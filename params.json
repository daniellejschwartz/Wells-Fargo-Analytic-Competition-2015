{"name":"Wells Fargo Analytics Competition 2015","tagline":"An interpretation of social media posts to determine bank qualities","body":"<!DOCTYPE html>\r\n<html>\r\n<head>\r\n<style>\r\nul {\r\n    list-style-type: none;\r\n    margin: 0;\r\n    padding: 0;\r\n    width: 200px;\r\n    background-color: #f1f1f1;\r\n}\r\n\r\nli a {\r\n    display: block;\r\n    color: #000;\r\n    padding: 8px 0 8px 16px;\r\n    text-decoration: none;\r\n}\r\n\r\n/* Change the link color on hover */\r\nli a:hover {\r\n    background-color: #555;\r\n    color: white;\r\n}\r\n</style>\r\n</head>\r\n<body>\r\n\r\n<h2>Navigation Bar</h2>\r\n\r\n<ul>\r\n  <li><a href=\"#Competition Overview\">Overview</a></li>\r\n  <li><a href=\"#Our Approach\">Approach</a></li>\r\n  <li><a href=\"#Cleaning the Data\">Cleaning</a></li>\r\n  <li><a href=\"#Exploratory Data Analysis\">Exploratory Analysis</a></li>\r\n</ul>\r\n\r\n</body>\r\n</html>\r\n\r\n## Competition Overview\r\nThe Wells Fargo Analytic Competition asked groups from Colleges and Universities to analyze a data set of Twitter and Facebook posts in order to develop solutions to the following questions:\r\n**What financial and bank topics* are consumers discussing in social media and what caused the consumers to post online about these topics?\r\nAre the topics and “substance” consistent across the industry or are they isolated to individual banks?**\r\n\r\nHere are a few sample posts from the original data set received from Wells Fargo:\r\n``` R\r\n118|11/1/2014|2014|11|facebook|- Name had BankB for over 8 years years \r\n    and no complaints at all! absolutely love BankB!\r\n123|8/27/2015|2015|8|twitter|- Name bouta get a banke account. BankB\r\n    got too many fees for Name.\r\n140|8/18/2014|2014|8|facebook|:(oh BankA.. every single time i do \r\n    something on accident it takes 5-10 business days to reverse!...\r\n    why cant i leave you?!.. why cant i just go to a banke...\r\n186|8/21/2015|2015|8|twitter|mannn * checks BankD account * INTERNET Ö\r\n```\r\n## Our Approach\r\nThe following procedure was developed to analyze the given data for posts specific to BankA, BankB, BankC, and BankD using the R programming language:\r\n\r\n![](http://i.imgur.com/YYxuhly.jpg)\r\n\r\n### Upload Packages\r\n``` R\r\n#Install Required Packages\r\nneeded <- c(\"tm\", \"SnowballC\", \"RColorBrewer\", \"ggplot2\", \"wordcloud\", \"biclust\", \"cluster\", \"igraph\", \"fpc\", \"slam\", \"plyr\", \"doMC\", \"stringr\", \"rpart\", \"randomForest\")\r\n#install.packages(needed, dependencies=TRUE) #This only needs to be done once\r\nlist = lapply(needed, require, character.only = TRUE) #This needs to be done every time you restart R\r\n```\r\n\r\n### Cleaning the Data\r\nAfter uploading the original data set into R and uploading the necessary packages (see above), the data was cleaned using the following methodology in order to create a data set that would be easier to work with and extract valuable information from. \r\n\r\n**1) Remove metadata**\r\n![Click to view metadata](http://i.imgur.com/A9KroLz.png\r\n**2) Remove non-ASCII characters**\r\n``` R\r\n#This function removes the non-ASCII characters from the text so that it can be analyzed \r\nremoveOffendingCharacters = function(df)\r\n{\r\n  df.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n  colnames(df.texts.clean) = 'FullText'\r\n  df$FullText = df.texts.clean$FullText\r\n  return(df)\r\n}\r\n```\r\n**3) Create function to remove white space, punctuation, numbers, stop words, change to lowercase**\r\n``` R\r\n#This function performs several operations to clean up the text, including:\r\n#Removing extra whitespace\r\n#Removing any punctuation with the exception of '_'\r\n#Removing numbers\r\n#Converting all text to lowercase\r\n#Removing common and meaningless words (stopwords)\r\ncleanText = function(df)\r\n{\r\n  require(tm)\r\n  docs = Corpus(VectorSource(df$FullText))\r\n  docs = tm_map(docs, stripWhitespace)\r\n  docs = tm_map(docs, removeWords, stopwords(\"english\"))  \r\n  docs = tm_map(docs, removeNumbers)   \r\n  docs = tm_map(docs, tolower) \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  removeSomePunct = function(x) gsub(\"[^[:alnum:][:blank:]_]\", \"\", x)\r\n  docs = tm_map(docs, content_transformer(removeSomePunct))\r\n  docs = tm_map(docs, stripWhitespace)  \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  \r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextClean = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n**4) Remove stems**\r\n``` R\r\n#This function \"stemms\" the document, meaning that it removes word endings like \"ing\", \"ed\", and \"es\"\r\n#For example, it will change the words \"analyze\", \"analyzing\", and \"analyzed\" to \"analyz\" \r\n#This makes it so that they are all treated as the same word\r\nstemText = function(df)\r\n{\r\n  require(tm)\r\n  require(SnowballC)\r\n  docs = Corpus(VectorSource(df$FullTextClean))\r\n  docs = tm_map(docs, stemDocument)\r\n  docs = tm_map(docs, PlainTextDocument)\r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextStemmed = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n### Exploratory Data Analysis\r\n**Sentiment analysis function**\r\n``` R\r\n#This function counts the number of positive words and negative words in a post\r\n#Positive and Negative words are imported from text files named positive-words.txt and negative-words.txt\r\n#The difference is the sentiment score\r\n#The difference divided by the number of words in the FullTextClean column is the sentiment density\r\n#This normalizes sentiment scores between very short posts (tweets) and very long posts (on facebook)\r\nsentimentAnalysis = function(df)\r\n{\r\n  pos <- scan('positive-words.txt',what='character',comment.char=';')\r\n  neg <- scan('negative-words.txt',what='character',comment.char=';')\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(df$FullTextClean, function(sentence, pos.words, neg.words) {\r\n    sentence = as.character(sentence)\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    words = unlist(word.list)\r\n    words.length = length(words)\r\n    \r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    if(words.length == 0) score.density = 0\r\n    else score.density = score/words.length\r\n    score.list = c(score, score.density)\r\n    \r\n    return(score.list)\r\n  }, pos, neg, .progress = 'text')\r\n  \r\n  scores.df = data.frame(SentimentScore=scores[,1], SentimentDensity=scores[,2])\r\n  df$SentimentScore = scores[,1]\r\n  df$SentimentDensity = scores[,2]\r\n  df$very.pos = as.numeric(df$SentimentScore >= 2)\r\n  df$very.neg = as.numeric(df$SentimentDensity <= -2)\r\n  return(df)\r\n}\r\n```\r\n\r\n**Analysis of frequent words**\r\n``` R\r\n#Create a Document Term Matrix to analyze the texts\r\ndocs = Corpus(VectorSource(df$FullTextClean))\r\ndtm = DocumentTermMatrix(docs)   \r\ndtm.sparse = dtms <- removeSparseTerms(dtm, 0.999)\r\ndtm.sparse.simple = as.simple_triplet_matrix(dtm.sparse) \r\n\r\n#Compute the word frequencies\r\nfreq <- sort(col_sums(dtm.sparse.simple), decreasing=TRUE)   \r\n#Remove the 9 most frequently occuring words because they are outliers\r\nfreq = freq[-c(1:9)]\r\nwf <- data.frame(word=names(freq), freq=freq)  \r\n\r\n#Make a bar graph of the words appearing more than 6000 times\r\nlibrary(ggplot2)   \r\np = ggplot(subset(wf, freq>6000), aes(word, freq))    \r\np = p + geom_bar(stat=\"identity\")   \r\np = p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np   \r\n```\r\n![](http://i.imgur.com/QaADvIn.png)\r\n\r\n**Example list of frequent words later used to manually define posts into the Bank Services category:**\r\n\r\nFirst Header | Second Header\r\n------------ | -------------\r\nContent from cell 1 | Content from cell 2\r\nContent in the first column | Content in the second column\r\n\r\nOnce the exploratory data analysis was complete, it was decided to separate the data set based on 4 topics: Customer Service (CS), Bank Services (S), Public Relations (PR), and Nonsense (NS). To analyze the substance of these topics for each bank, we first needed to separate the posts by bank mentions. \r\n### Separating the Posts by Bank\r\n``` R\r\n#This function analyzes the dataset to identify which banks are mentioned in each post.\r\n#It creates 5 new variables (for banks A-E)\r\n#The field has a 0 if the bank is not mentioned, and a 1 if the bank is mentioned.\r\n#It also creates a field that counts the number of banks mentioned in the post.\r\n#Some posts mention 0 banks, most mention 1, and a few mention multiple\r\nbankMentions = function(df)\r\n{\r\n  df$BankA = 0\r\n  df$BankB = 0\r\n  df$BankC = 0\r\n  df$BankD = 0\r\n  df$BankE = 0\r\n  df$NumBanks = 0\r\n  for( i in 1:nrow(df))\r\n  {\r\n    if (grepl(\"BankA\",df$FullText[i]))  df$BankA[i] = 1\r\n    if (grepl(\"BankB\",df$FullText[i]))  df$BankB[i] = 1\r\n    if (grepl(\"BankC\",df$FullText[i]))  df$BankC[i] = 1\r\n    if (grepl(\"BankD\",df$FullText[i]))  df$BankD[i] = 1\r\n    if (grepl(\"banke\",df$FullText[i]))  df$BankE[i] = 1\r\n    df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]\r\n  }\r\n  \r\n  return(df)\r\n}\r\n```\r\n\r\n\r\n### Bag-of-Words Random Forest Classification Algorithm\r\nIn order to classify the data set into the constructed categories (Customer Service, Bank Services, Public Relations, and Nonsense) using a bag-of-words random forest classification algorithm, a sample of 600 posts were labeled by category and continuously applied to the dataset. For example, if a post mentions “atm” or “online banking,” it would be labeled in the Bank Services category. \r\n\r\nThe program then learned from the sample data to create predictions of category for each post. During cross validation, the accuracy of predictions was found to be approximately 77%. The classification tree and naive bayes classification algorithms were also tested, but both had much lower accuracy. A larger training data set (>600 posts)would likely improve accuracy further.\r\n**\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}