{"name":"Wells Fargo Analytics Competition 2015","tagline":"An interpretation of social media posts to determine bank qualities","body":"# Competition Overview\r\nThe Wells Fargo Analytic Competition asked groups from Colleges and Universities to analyze a data set of Twitter and Facebook posts in order to develop solutions to the following questions:\r\n**What financial and bank topics* are consumers discussing in social media and what caused the consumers to post online about these topics?\r\nAre the topics and “substance” consistent across the industry or are they isolated to individual banks?**\r\n\r\nHere are a few sample posts from the original data set received from Wells Fargo:\r\n``` R\r\n118|11/1/2014|2014|11|facebook|- Name had BankB for over 8 years years \r\n    and no complaints at all! absolutely love BankB!\r\n123|8/27/2015|2015|8|twitter|- Name bouta get a banke account. BankB\r\n    got too many fees for Name.\r\n140|8/18/2014|2014|8|facebook|:(oh BankA.. every single time i do \r\n    something on accident it takes 5-10 business days to reverse!...\r\n    why cant i leave you?!.. why cant i just go to a banke...\r\n186|8/21/2015|2015|8|twitter|mannn * checks BankD account * INTERNET Ö\r\n```\r\n# Our Approach\r\nThe following procedure was developed to analyze the given data for posts specific to BankA, BankB, BankC, and BankD using the R programming language:\r\n\r\n![](http://i.imgur.com/YYxuhly.jpg)\r\n\r\n### Upload Packages\r\n``` R\r\n#Install Required Packages\r\nneeded <- c(\"tm\", \"SnowballC\", \"RColorBrewer\", \"ggplot2\", \"wordcloud\", \"biclust\", \"cluster\", \"igraph\", \"fpc\", \"slam\", \"plyr\", \"doMC\", \"stringr\", \"rpart\", \"randomForest\")\r\n#install.packages(needed, dependencies=TRUE) #This only needs to be done once\r\nlist = lapply(needed, require, character.only = TRUE) #This needs to be done every time you restart R\r\n```\r\n\r\n### Cleaning the Data\r\nAfter uploading the original data set into R and uploading the necessary packages (see above), the data was cleaned using the following methodology in order to create a data set that would be easier to work with and extract valuable information from. \r\n\r\n**1) Remove metadata**\r\n![Click to view metadata](http://i.imgur.com/A9KroLz.png\r\n\r\n**2) Remove non-ASCII characters**\r\n``` R\r\n#This function removes the non-ASCII characters from the text so that it can be analyzed \r\nremoveOffendingCharacters = function(df)\r\n{\r\n  df.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n  colnames(df.texts.clean) = 'FullText'\r\n  df$FullText = df.texts.clean$FullText\r\n  return(df)\r\n}\r\n```\r\n**3) Create function to remove white space, punctuation, numbers, stop words, change to lowercase**\r\n``` R\r\n#This function performs several operations to clean up the text, including:\r\n#Removing extra whitespace\r\n#Removing any punctuation with the exception of '_'\r\n#Removing numbers\r\n#Converting all text to lowercase\r\n#Removing common and meaningless words (stopwords)\r\ncleanText = function(df)\r\n{\r\n  require(tm)\r\n  docs = Corpus(VectorSource(df$FullText))\r\n  docs = tm_map(docs, stripWhitespace)\r\n  docs = tm_map(docs, removeWords, stopwords(\"english\"))  \r\n  docs = tm_map(docs, removeNumbers)   \r\n  docs = tm_map(docs, tolower) \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  removeSomePunct = function(x) gsub(\"[^[:alnum:][:blank:]_]\", \"\", x)\r\n  docs = tm_map(docs, content_transformer(removeSomePunct))\r\n  docs = tm_map(docs, stripWhitespace)  \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  \r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextClean = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n**4) Remove stems**\r\n``` R\r\n#This function \"stemms\" the document, meaning that it removes word endings like \"ing\", \"ed\", and \"es\"\r\n#For example, it will change the words \"analyze\", \"analyzing\", and \"analyzed\" to \"analyz\" \r\n#This makes it so that they are all treated as the same word\r\nstemText = function(df)\r\n{\r\n  require(tm)\r\n  require(SnowballC)\r\n  docs = Corpus(VectorSource(df$FullTextClean))\r\n  docs = tm_map(docs, stemDocument)\r\n  docs = tm_map(docs, PlainTextDocument)\r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextStemmed = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n### Exploratory Data Analysis\r\nWe began our analysis by exploring the data, examining common word frequencies, and identifying clusters of words. These preliminary results helped direct us in our classification and sentiment analyses.\r\n\r\n**Analysis of frequent words**\r\n``` R\r\n#Create a Document Term Matrix to analyze the texts\r\ndocs = Corpus(VectorSource(df$FullTextClean))\r\ndtm = DocumentTermMatrix(docs)   \r\ndtm.sparse = dtms <- removeSparseTerms(dtm, 0.999)\r\ndtm.sparse.simple = as.simple_triplet_matrix(dtm.sparse) \r\n\r\n#Compute the word frequencies\r\nfreq <- sort(col_sums(dtm.sparse.simple), decreasing=TRUE)   \r\n#Remove the 9 most frequently occuring words because they are outliers\r\nfreq = freq[-c(1:9)]\r\nwf <- data.frame(word=names(freq), freq=freq)  \r\n\r\n#Make a bar graph of the words appearing more than 6000 times\r\nlibrary(ggplot2)   \r\np = ggplot(subset(wf, freq>6000), aes(word, freq))    \r\np = p + geom_bar(stat=\"identity\")   \r\np = p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np   \r\n```\r\n![](http://i.imgur.com/QaADvIn.png)\r\n\r\n``` R\r\n#Create a word cloud of the 60 most frequently occuring words\r\nlibrary(wordcloud)   \r\nset.seed(145)   \r\nwordcloud(names(freq), freq, max.words = 60, scale=c(5, .1), colors=brewer.pal(6, \"Dark2\"))\r\n```\r\n![](http://i.imgur.com/jp97IGK.png)\r\n\r\nOnce the exploratory data analysis was complete, it was decided to separate the data set based on 4 topics: Customer Service (CS), Bank Services (S), Public Relations (PR), and Nonsense (NS). \r\n\r\n**Example list of frequent words later used to manually define posts into the Bank Services category:**\r\n\r\n**account** | **card** | **money** | **email**\r\n----------| ---------- | -----------  | ------------\r\n**rate** | **check** | **credit** | **cash**\r\n**pay** | **buy** | **loan** | **atm**\r\n**deposit** | **fee** | **online** | **debit**\r\n\r\n**Creation of sentiment analysis function**\r\n``` R\r\n#This function counts the number of positive words and negative words in a post\r\n#Positive and Negative words are imported from text files named positive-words.txt and negative-words.txt\r\n#The difference is the sentiment score\r\n#The difference divided by the number of words in the FullTextClean column is the sentiment density\r\n#This normalizes sentiment scores between very short posts (tweets) and very long posts (on facebook)\r\nsentimentAnalysis = function(df)\r\n{\r\n  pos <- scan('positive-words.txt',what='character',comment.char=';')\r\n  neg <- scan('negative-words.txt',what='character',comment.char=';')\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(df$FullTextClean, function(sentence, pos.words, neg.words) {\r\n    sentence = as.character(sentence)\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    words = unlist(word.list)\r\n    words.length = length(words)\r\n    \r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    if(words.length == 0) score.density = 0\r\n    else score.density = score/words.length\r\n    score.list = c(score, score.density)\r\n    \r\n    return(score.list)\r\n  }, pos, neg, .progress = 'text')\r\n  \r\n  scores.df = data.frame(SentimentScore=scores[,1], SentimentDensity=scores[,2])\r\n  df$SentimentScore = scores[,1]\r\n  df$SentimentDensity = scores[,2]\r\n  df$very.pos = as.numeric(df$SentimentScore >= 2)\r\n  df$very.neg = as.numeric(df$SentimentDensity <= -2)\r\n  return(df)\r\n}\r\n```\r\n### Bag-of-Words Random Forest Classification Algorithm\r\nIn order to classify the data set into the constructed categories (Customer Service, Bank Services, Public Relations, and Nonsense) using a bag-of-words random forest classification algorithm, a sample of 600 posts were labeled by category and continuously applied to the dataset. For example, if a post mentions “atm” or “online banking,” it would be labeled in the Bank Services category. \r\n\r\nThe program then learned from the sample data to create predictions of category for each post. During cross validation, the accuracy of predictions was found to be approximately 77%. The classification tree and naive bayes classification algorithms were also tested, but both had much lower accuracy. A larger training data set (>600 posts)would likely improve accuracy further.\r\n\r\n![](http://i.imgur.com/8u5rA0g.png)\r\n\r\n### Overall Sentiment\r\n\r\n*Boxplot of Sentiment Density Scores for each Topic. Nonsense posts made up the majority of all posts and have the greatest spread of sentiment.*\r\n![](http://i.imgur.com/cXxmn7v.png)\r\n\r\n*Average Sentiment Density Scores for each Topic. The nonsense posts provide a baseline sentiment score against which we can compare the other topics. Across all posts, scores for bank services average negative and scores for customer service and public relations average positive.*\r\n![](http://i.imgur.com/Qp9dhJp.png)\r\n\r\nOn average, customers have positive sentiments for customer service related posts and negative sentiments for bank services related posts. Therefore, the substance for both customer service and bank services is universal across the industry. However, the degree of sentiment associated with each category varies by bank. While customers have generally negative interactions with banking services, banks appear to be able to compensate for these issues with generally positive interactions with public relations and customer service. Banks should focus on improving their Bank Services as listed above.\r\n\r\n### Separating the Posts by Bank\r\n``` R\r\n#This function analyzes the dataset to identify which banks are mentioned in each post.\r\n#It creates 5 new variables (for banks A-E)\r\n#The field has a 0 if the bank is not mentioned, and a 1 if the bank is mentioned.\r\n#It also creates a field that counts the number of banks mentioned in the post.\r\n#Some posts mention 0 banks, most mention 1, and a few mention multiple\r\nbankMentions = function(df)\r\n{\r\n  df$BankA = 0\r\n  df$BankB = 0\r\n  df$BankC = 0\r\n  df$BankD = 0\r\n  df$BankE = 0\r\n  df$NumBanks = 0\r\n  for( i in 1:nrow(df))\r\n  {\r\n    if (grepl(\"BankA\",df$FullText[i]))  df$BankA[i] = 1\r\n    if (grepl(\"BankB\",df$FullText[i]))  df$BankB[i] = 1\r\n    if (grepl(\"BankC\",df$FullText[i]))  df$BankC[i] = 1\r\n    if (grepl(\"BankD\",df$FullText[i]))  df$BankD[i] = 1\r\n    if (grepl(\"banke\",df$FullText[i]))  df$BankE[i] = 1\r\n    df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]\r\n  }\r\n  \r\n  return(df)\r\n}\r\n```\r\n*Average Sentiment Density Scores by Topic and Bank*\r\n![](http://i.imgur.com/8J6V5FE.png)\r\n\r\nSentiment scores varied in magnitude and direction by bank for each topic. For example, Bank B had the most positive sentiments for customer service and pr, but the most negative sentiment for bank services. It is possible that Bank B is forced to compensate for poor banking services with more customer service and public relations. This information is useful for individual banks to see how their services compare to their competitors. \r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}