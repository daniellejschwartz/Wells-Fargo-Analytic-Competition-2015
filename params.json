{"name":"Wells Fargo Analytics Competition 2015","tagline":"An interpretation of social media posts to determine bank qualities","body":"# Competition Overview\r\nThe Wells Fargo Analytic Competition asked groups from Colleges and Universities to analyze a data set of Twitter and Facebook posts in order to develop solutions to the following questions:\r\n**What financial and bank topics* are consumers discussing in social media and what caused the consumers to post online about these topics?\r\nAre the topics and “substance” consistent across the industry or are they isolated to individual banks?**\r\n\r\nHere are a few sample posts from the original data set received from Wells Fargo:\r\n``` R\r\n118|11/1/2014|2014|11|facebook|- Name had BankB for over 8 years years \r\n    and no complaints at all! absolutely love BankB!\r\n123|8/27/2015|2015|8|twitter|- Name bouta get a banke account. BankB\r\n    got too many fees for Name.\r\n140|8/18/2014|2014|8|facebook|:(oh BankA.. every single time i do \r\n    something on accident it takes 5-10 business days to reverse!...\r\n    why cant i leave you?!.. why cant i just go to a banke...\r\n186|8/21/2015|2015|8|twitter|mannn * checks BankD account * INTERNET Ö\r\n```\r\n# Our Approach\r\nThe following procedure was developed to analyze the given data for posts specific to BankA, BankB, BankC, and BankD using the R programming language:\r\n\r\n![](http://i.imgur.com/YYxuhly.jpg)\r\n\r\n## Upload Packages\r\n``` R\r\n#Install Required Packages\r\nneeded <- c(\"tm\", \"SnowballC\", \"RColorBrewer\", \"ggplot2\", \"wordcloud\", \"biclust\", \"cluster\", \"igraph\", \"fpc\", \"slam\", \"plyr\", \"doMC\", \"stringr\", \"rpart\", \"randomForest\")\r\n#install.packages(needed, dependencies=TRUE) #This only needs to be done once\r\nlist = lapply(needed, require, character.only = TRUE) #This needs to be done every time you restart R\r\n```\r\n\r\n## Metadata in data set\r\n![](http://i.imgur.com/rsja9Zw.png)\r\n\r\n## Cleaning the Data\r\nAfter uploading the original data set into R and uploading the necessary packages (see above), the data was cleaned using the following methodology in order to create a data set that would be easier to work with and extract valuable information from. \r\n\r\n**1) Remove non-ASCII characters**\r\n``` R\r\n#This function removes the non-ASCII characters from the text so that it can be analyzed \r\nremoveOffendingCharacters = function(df)\r\n{\r\n  df.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n  colnames(df.texts.clean) = 'FullText'\r\n  df$FullText = df.texts.clean$FullText\r\n  return(df)\r\n}\r\n```\r\n**2) Create function to remove white space, punctuation, numbers, stop words, change to lowercase**\r\n``` R\r\n#This function performs several operations to clean up the text, including:\r\n#Removing extra whitespace\r\n#Removing any punctuation with the exception of '_'\r\n#Removing numbers\r\n#Converting all text to lowercase\r\n#Removing common and meaningless words (stopwords)\r\ncleanText = function(df)\r\n{\r\n  require(tm)\r\n  docs = Corpus(VectorSource(df$FullText))\r\n  docs = tm_map(docs, stripWhitespace)\r\n  docs = tm_map(docs, removeWords, stopwords(\"english\"))  \r\n  docs = tm_map(docs, removeNumbers)   \r\n  docs = tm_map(docs, tolower) \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  removeSomePunct = function(x) gsub(\"[^[:alnum:][:blank:]_]\", \"\", x)\r\n  docs = tm_map(docs, content_transformer(removeSomePunct))\r\n  docs = tm_map(docs, stripWhitespace)  \r\n  docs = tm_map(docs, PlainTextDocument)\r\n  \r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextClean = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n**3) Remove stems**\r\n``` R\r\n#This function \"stemms\" the document, meaning that it removes word endings like \"ing\", \"ed\", and \"es\"\r\n#For example, it will change the words \"analyze\", \"analyzing\", and \"analyzed\" to \"analyz\" \r\n#This makes it so that they are all treated as the same word\r\nstemText = function(df)\r\n{\r\n  require(tm)\r\n  require(SnowballC)\r\n  docs = Corpus(VectorSource(df$FullTextClean))\r\n  docs = tm_map(docs, stemDocument)\r\n  docs = tm_map(docs, PlainTextDocument)\r\n  df.texts.cleaner = data.frame(text=unlist(sapply(docs, `[`, \"content\")), stringsAsFactors=F)\r\n  colnames(df.texts.cleaner) = 'FullText'\r\n  df$FullTextStemmed = df.texts.cleaner$FullText\r\n  return(df)\r\n}\r\n```\r\n\r\n## Exploratory Data Analysis\r\nWe began our analysis by exploring the data, examining common word frequencies, and identifying clusters of words. These preliminary results helped direct us in our classification and sentiment analyses.\r\n\r\n**Analysis of frequent words**\r\nThe text was analyzed for the most frequent words, and a bar graph and word cloud were constructed for representation. The top 9 most frequent words were removed as they were outliers in the data set. \r\n``` R\r\n#Create a Document Term Matrix to analyze the texts\r\ndocs = Corpus(VectorSource(df$FullTextClean))\r\ndtm = DocumentTermMatrix(docs)   \r\ndtm.sparse = dtms <- removeSparseTerms(dtm, 0.999)\r\ndtm.sparse.simple = as.simple_triplet_matrix(dtm.sparse) \r\n\r\n#Compute the word frequencies\r\nfreq <- sort(col_sums(dtm.sparse.simple), decreasing=TRUE)   \r\n#Remove the 9 most frequently occuring words because they are outliers\r\nfreq = freq[-c(1:9)]\r\nwf <- data.frame(word=names(freq), freq=freq)  \r\n\r\n#Make a bar graph of the words appearing more than 6000 times\r\nlibrary(ggplot2)   \r\np = ggplot(subset(wf, freq>6000), aes(word, freq))    \r\np = p + geom_bar(stat=\"identity\")   \r\np = p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np   \r\n```\r\n![](http://i.imgur.com/QaADvIn.png)\r\n\r\n``` R\r\n#Create a word cloud of the 60 most frequently occurring words\r\nlibrary(wordcloud)   \r\nset.seed(145)   \r\nwordcloud(names(freq), freq, max.words = 60, scale=c(5, .1), colors=brewer.pal(6, \"Dark2\"))\r\n```\r\n![](http://i.imgur.com/jp97IGK.png)\r\n\r\nOnce the exploratory data analysis was complete, it was decided to separate the data set based on 4 topics: Customer Service (CS), Bank Services (S), Public Relations (PR), and Nonsense (NS). \r\n\r\n**Example list of frequent words later used to manually define posts into the Bank Services category:**\r\n\r\n**account** | **card** | **money** | **email**\r\n----------| ---------- | -----------  | ------------\r\n**rate** | **check** | **credit** | **cash**\r\n**pay** | **buy** | **loan** | **atm**\r\n**deposit** | **fee** | **online** | **debit**\r\n\r\nRunning a sentiment analysis for each category was decided to be the best possible way to determine whether categories had a positive or negative connotation on social media outlets like Twitter and Facebook. The following function was created to run sentiment on the categories with ease. \r\n\r\n**Creation of sentiment analysis function**\r\n``` R\r\n#This function counts the number of positive words and negative words in a post\r\n#Positive and Negative words are imported from text files named positive-words.txt and negative-words.txt\r\n#The difference is the sentiment score\r\n#The difference divided by the number of words in the FullTextClean column is the sentiment density\r\n#This normalizes sentiment scores between very short posts (tweets) and very long posts (on facebook)\r\nsentimentAnalysis = function(df)\r\n{\r\n  pos <- scan('positive-words.txt',what='character',comment.char=';')\r\n  neg <- scan('negative-words.txt',what='character',comment.char=';')\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(df$FullTextClean, function(sentence, pos.words, neg.words) {\r\n    sentence = as.character(sentence)\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    words = unlist(word.list)\r\n    words.length = length(words)\r\n    \r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    if(words.length == 0) score.density = 0\r\n    else score.density = score/words.length\r\n    score.list = c(score, score.density)\r\n    \r\n    return(score.list)\r\n  }, pos, neg, .progress = 'text')\r\n  \r\n  scores.df = data.frame(SentimentScore=scores[,1], SentimentDensity=scores[,2])\r\n  df$SentimentScore = scores[,1]\r\n  df$SentimentDensity = scores[,2]\r\n  df$very.pos = as.numeric(df$SentimentScore >= 2)\r\n  df$very.neg = as.numeric(df$SentimentDensity <= -2)\r\n  return(df)\r\n}\r\n```\r\nAs you can see, the sentiment density scores have been normalized for length of post. Therefore, when comparing tweets (140 character max) and Facebook posts (63,206 character max) the sentiment will not be directly influenced by the length of the posts. \r\n \r\n## Bag-of-Words Random Forest Classification Algorithm\r\nIn order to classify the data set into the constructed categories (Customer Service, Bank Services, Public Relations, and Nonsense) using a bag-of-words random forest classification algorithm, a sample of 600 posts were labeled manually by category and continuously applied to the dataset. For example, if a post mentions “atm” or “online banking,” it would be labeled in the Bank Services category. \r\n\r\nThe program then learned from the sample data to create predictions of category for each post. During cross validation, the accuracy of predictions was found to be approximately 77%. The classification tree and naive bayes classification algorithms were also tested, but both had much lower accuracy. A larger training data set (>600 posts) would likely improve accuracy further.\r\n\r\n![](http://i.imgur.com/8u5rA0g.png)\r\n\r\n## Overall Sentiment Results\r\n\r\n*Boxplot of Sentiment Density Scores for each Topic: Nonsense posts made up the majority of all posts and have the greatest spread of sentiment.*\r\n![](http://i.imgur.com/cXxmn7v.png)\r\n\r\n*Average Sentiment Density Scores for each Topic: The nonsense posts provide a baseline sentiment score against which we can compare the other topics. Across all posts, scores for bank services average negative and scores for customer service and public relations average positive.*\r\n![](http://i.imgur.com/Qp9dhJp.png)\r\n\r\nOn average, customers have positive sentiments for customer service related posts and negative sentiments for bank services related posts. Therefore, the substance for both customer service and bank services is universal across the industry. However, the degree of sentiment associated with each category varies by bank. While customers have generally negative interactions with banking services, banks appear to be able to compensate for these issues with generally positive interactions with public relations and customer service. Banks should focus on improving their Bank Services as listed above.\r\n\r\n## Separating the Posts by Bank\r\nIn order to determine if the sentiment for Customer Service, Bank Services, and Public Relations are universal across all banks, the posts need to be organized into individual data frames for each bank. To do this, a function to identify bank mentions in posts was created, and this information was then used to assign posts mentioning banks to their respective data frame. If a posts mentions more then one bank, it will be present in multiple data frames.\r\n\r\n``` R\r\n#The field has a 0 if the bank is not mentioned, and a 1 if the bank is mentioned.\r\n#It also creates a field that counts the number of banks mentioned in the post.\r\n#Some posts mention 0 banks, most mention 1, and a few mention multiple\r\nbankMentions = function(df)\r\n{\r\n  df$BankA = 0\r\n  df$BankB = 0\r\n  df$BankC = 0\r\n  df$BankD = 0\r\n  df$BankE = 0\r\n  df$NumBanks = 0\r\n  for( i in 1:nrow(df))\r\n  {\r\n    if (grepl(\"BankA\",df$FullText[i]))  df$BankA[i] = 1\r\n    if (grepl(\"BankB\",df$FullText[i]))  df$BankB[i] = 1\r\n    if (grepl(\"BankC\",df$FullText[i]))  df$BankC[i] = 1\r\n    if (grepl(\"BankD\",df$FullText[i]))  df$BankD[i] = 1\r\n    if (grepl(\"banke\",df$FullText[i]))  df$BankE[i] = 1\r\n    df$NumBanks[i] = df$BankA[i] + df$BankB[i] + df$BankC[i] + df$BankD[i] + df$BankE[i]\r\n  }\r\n  \r\n  return(df)\r\n}\r\n\r\n#Creating Bank Data Frames\r\nbankAindices = which(df$BankA ==1)\r\ndf.BankA = df[bankAindices,]\r\nmeanscore.BankA = tapply(df.BankA$SentimentDensity, df.BankA$Topic, mean)\r\ndf.plot.A = data.frame(Bank = c(\"BankA\", \"BankA\", \"BankA\", \"BankA\"), Topic=names(meanscore.BankA), meanscore=meanscore.BankA)\r\n\r\nbankBindices = which(df$BankB ==1)\r\ndf.BankB = df[bankBindices,]\r\nmeanscore.BankB = tapply(df.BankB$SentimentDensity, df.BankB$Topic, mean)\r\ndf.plot.B = data.frame(Bank = c(\"BankB\", \"BankB\", \"BankB\",\"BankB\"), Topic=names(meanscore.BankB), meanscore=meanscore.BankB)\r\n\r\nbankCindices = which(df$BankC ==1)\r\ndf.BankC = df[bankCindices,]\r\nmeanscore.BankC = tapply(df.BankC$SentimentDensity, df.BankC$Topic, mean)\r\ndf.plot.C = data.frame(Bank = c(\"BankC\", \"BankC\", \"BankC\",\"BankC\"), Topic=names(meanscore.BankC), meanscore=meanscore.BankC)\r\n\r\nbankDindices = which(df$BankD ==1)\r\ndf.BankD = df[bankDindices,]\r\nmeanscore.BankD = tapply(df.BankD$SentimentDensity, df.BankD$Topic, mean)\r\ndf.plot.D = data.frame(Bank = c(\"BankD\", \"BankD\", \"BankD\", \"BankD\"), Topic=names(meanscore.BankD), meanscore=meanscore.BankD)\r\n```\r\n*Number of posts mentioning each bank*\r\n![](http://i.imgur.com/8J6V5FE.png)\r\n\r\n## Bank Specific Sentiment Results\r\nThe previously constructed sentiment function was run on each bank data frame. \r\n\r\n*Average Sentiment Density Scores by Topic and Bank*\r\n\r\n![](http://i.imgur.com/gtt9umK.png)\r\n\r\nBank A\r\n\r\n> **Customer Service:** Positive\r\n> **Bank Services:** Negative\r\n> **Public Relations:** Positive\r\n\r\nBank B\r\n\r\n> **Customer Service:** Positive\r\n> **Bank Services:** Negative\r\n> **Public Relations:** Positive\r\n\r\nBank C\r\n\r\n> **Customer Service:** Positive\r\n> **Bank Services:** Negative\r\n> **Public Relations:** Negative\r\n\r\nBank D\r\n\r\n> **Customer Service:** Positive\r\n> **Bank Services:** Negative\r\n> **Public Relations:** Negative\r\n\r\nSentiment scores varied in magnitude and direction by bank for each topic. For example, Bank B had the most positive sentiments for Customer Service and Public Relations, but the most negative sentiment for bank services. It is possible that Bank B is forced to compensate for poor banking services with more customer service and public relations. This information is useful for individual banks to see how their services compare to their competitors. \r\n\r\n## Mention Analysis ##\r\n\r\nAssuming all posts mentioning the string \"ATM\" were labelled in the Bank Services category, we can investigate ATM mentions in each bank data frame to determine the overall impact of ATM's on the Bank Services sentiment for each bank. As listed above, all of the banks were concluded to have negative sentiment associated with their Bank Services. \r\n\r\n*Code for determining # of ATM mentions in each bank, then creating an ATM impact score by summing the mentions and dividing by # of overall posts about the bank. The creation of the impact score removes any bias contributed to the differing number of posts per bank.*\r\n``` R\r\n#creates a column in each bank data set counting the number of times the string \"atm\"\r\n#is mentioned\r\ndf.BankA$BankA.atm = 0\r\ndf.BankB$BankB.atm = 0\r\ndf.BankC$BankC.atm = 0\r\ndf.BankD$BankD.atm = 0\r\n\r\n\r\nfor( i in 1:nrow(df.BankA))\r\n{\r\n  if (grepl(\"atm\",df.BankA$FullText[i]))  df.BankA$BankA.atm[i] = 1\r\n}\r\nfor( i in 1:nrow(df.BankB))\r\n{\r\n  if (grepl(\"atm\",df.BankB$FullText[i]))  df.BankB$BankB.atm[i] = 1\r\n}\r\nfor( i in 1:nrow(df.BankC))\r\n{\r\n  if (grepl(\"atm\",df.BankC$FullText[i]))  df.BankC$BankC.atm[i] = 1\r\n}\r\nfor( i in 1:nrow(df.BankD))\r\n{\r\n  if (grepl(\"atm\",df.BankD$FullText[i]))  df.BankD$BankD.atm[i] = 1\r\n}\r\n\r\n#counts the number of times each posts about each bank mentions atm\r\nbankAatmMentions = sum(df.BankA$BankA.atm)\r\nbankBatmMentions = sum(df.BankB$BankB.atm)\r\nbankCatmMentions = sum(df.BankC$BankC.atm)\r\nbankDatmMentions = sum(df.BankD$BankD.atm)\r\n\r\n#atm mention score regulated for number of posts per bank\r\n#multiply by 100 to receive % of posts\r\nAatmMentionScore= bankAatmMentions/bankAMentions\r\nBatmMentionScore= bankBatmMentions/bankAMentions\r\nCatmMentionScore= bankCatmMentions/bankCMentions\r\nDatmMentionScore= bankDatmMentions/bankDMentions\r\n\r\n\r\nBank = c(\"BankA\", \"BankB\", \"BankC\", \"BankD\")\r\nMentions = c(bankAatmMentions, bankBatmMentions, bankCatmMentions, bankDatmMentions)\r\nbankatmMentions = data.frame(Bank, Mentions)\r\np = ggplot(bankatmMentions, aes(Bank, Mentions))\r\np = p + geom_bar(stat=\"identity\")   \r\np = p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np\r\n\r\n\r\nBank = c(\"BankA\", \"BankB\", \"BankC\", \"BankD\")\r\nMentions = c(AatmMentionScore, BatmMentionScore, CatmMentionScore, DatmMentionScore)\r\nbankatmMentions = data.frame(Bank, Mentions)\r\np = ggplot(bankatmMentions, aes(Bank, Mentions),fill=Bank)\r\np = p + geom_bar(stat=\"identity\")   \r\np = p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np\r\n```\r\n\r\n*Number of posts mentioning ATM's by bank*\r\n\r\n![](http://i.imgur.com/SpREc1h.png)\r\n\r\n*ATM impact score by bank*\r\n\r\n![](http://i.imgur.com/nTIyjss.png)\r\n\r\nThe graph tells us that ATM's did not have much of a presence in the posts related to Bank Services (the largest percentage is Bank B with 1.7% of the posts mentioning ATM's.) This shows that ATM's may not be the largest service problem for the banks. However, you can see that there is greater prevalence of ATM mentions for banks A and B, so they may want to place more attention on this area of service compared to bank C or D. \r\n\r\n**This method of analysis is useful to banks as it can be done for multiple services to check the status compared to other banks or specific services if a bank want to check out how a specific service is being reviewed on social media.** \r\n\r\nFor another example, lets check the posts for each bank mentioning \"accounts\" using the same methodology.\r\n\r\n*Account impact score by bank*\r\n\r\n![](http://i.imgur.com/0YNLqSs.png)\r\n\r\nHere we can see that posts about accounts have a greater impact on the sentiment score, as there are more mentions of account services than the ATM services. Banks A, B, and D all have high scores for account mentions, with Bank B being the greatest at 8.8%. Changing the approach to current account processes is definitely something these banks should take into account when looking to improve their Bank Services. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}